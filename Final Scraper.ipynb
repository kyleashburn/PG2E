{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc142d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f2e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bef4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1b49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Item, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655de099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class comment(Item):\n",
    "    comment_author = Field()\n",
    "    comment_id = Field()\n",
    "    timestamp = Field() \n",
    "    comment_text = Field()\n",
    "    num_likes = Field() # nullable -> default value is 0\n",
    "    parent_id = Field() # nullable -> default value should be \"N/A\"+\n",
    "    post_num = Field() # the post idea of the parent -> kind of important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd48b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class book_chapter(Item):\n",
    "    book = Field()\n",
    "    chapt_title = Field()\n",
    "    published = Field()\n",
    "    updated = Field() # this one can be empty -> makes sense to me\n",
    "    chapt_author = Field()\n",
    "    chapt_text = Field()\n",
    "    num_comments = Field()\n",
    "    num_likes = Field()\n",
    "    post_num = Field()\n",
    "    liked_by = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5440bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines!\n",
    "from scrapy.utils.serialize import ScrapyJSONEncoder # this imports a utility that enables us to serialize the item\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from itemloaders.processors import Compose, Join, MapCompose\n",
    "\n",
    "import shortuuid\n",
    "\n",
    "class chapter_proc_pipeline(object):\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        # first to check the type of item we're working with here\n",
    "        if not isinstance(item, book_chapter):\n",
    "            return item # breaking if it's not a book_chapter\n",
    "        \n",
    "        standard_proc = Compose(str.strip, lambda v: unidecode(v), stop_on_none=False) # standard proc\n",
    "        \n",
    "        text_proc = Compose(Join(), str.strip, lambda v: unidecode(v), stop_on_none=False) # text proc\n",
    "        \n",
    "        likes_proc = Compose(lambda v: int(v) if v else 0, stop_on_none=False) # processes the number of likes\n",
    "        \n",
    "        num_comment_proc = Compose(lambda v: v.split()[0], int, stop_on_none=False) # proc for num comments\n",
    "        \n",
    "        liked_by_proc = MapCompose(str.strip, lambda v: unidecode(v), stop_on_none=False) # stand proc applied to each item\n",
    "              \n",
    "\n",
    "        # applying my functions\n",
    "        item[\"book\"] = standard_proc(item[\"book\"]) # book (number/title)\n",
    "        \n",
    "        item[\"chapt_title\"] = standard_proc(item[\"chapt_title\"]) # chapter title\n",
    "        \n",
    "        item[\"chapt_author\"] = standard_proc(item[\"chapt_author\"]) # chapter author\n",
    "        \n",
    "        item[\"chapt_text\"] = text_proc(item[\"chapt_text\"]) # chapter text joined and cleaned\n",
    "        \n",
    "        item[\"num_comments\"] = num_comment_proc(item[\"num_comments\"]) # number of comments\n",
    "        \n",
    "        item[\"num_likes\"] = likes_proc(item[\"num_likes\"]) # number of likes\n",
    "        \n",
    "        item[\"liked_by\"] = liked_by_proc(item[\"liked_by\"]) # liked by\n",
    "        \n",
    "        item[\"post_num\"] = standard_proc(item[\"post_num\"]) # minimal proc on post num\n",
    "               \n",
    "        # no proc for published & updated b/c datetime format;\n",
    "        \n",
    "        return item \n",
    "        \n",
    "# the comment proc pipeline\n",
    "class comment_proc_pipeline(object):\n",
    "       \n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        # checking if it's not a comment\n",
    "        if not isinstance(item, comment):\n",
    "            return item # just breaking this function & not processing the item if it doesn't belong\n",
    "        \n",
    "        # functions to process everything\n",
    "        author_proc = Compose(str.strip, lambda v: unidecode(v), stop_on_none=False) # author proc\n",
    "\n",
    "        # example of my text cleaning step\n",
    "        text_proc = Compose(Join(), str.strip, lambda v: unidecode(v), stop_on_none=False) # comment text proc\n",
    "\n",
    "        # example of dealing with the comment id\n",
    "        comment_proc = Compose(lambda v: v.split(\"-\")[1], str.strip, stop_on_none=False) # comment id proc\n",
    "\n",
    "        likes_proc = Compose(lambda v: v[0].split()[0] if v else \"0\", int, stop_on_none=False) # likes proc; if none, 0\n",
    "\n",
    "        parent_proc = Compose(lambda v: comment_proc(v) if v else None, stop_on_none=False) # parent id; else None\n",
    "        \n",
    "        # applying the functions we just made\n",
    "        item[\"comment_author\"] = author_proc(item[\"comment_author\"]) # cleaning up the author (to be safe)\n",
    "        \n",
    "        item[\"comment_id\"] =comment_proc(item[\"comment_id\"]) # cleanning up comment_id\n",
    "       \n",
    "        item[\"comment_text\"] =  text_proc(item[\"comment_text\"]) # cleaning up the comment text\n",
    "    \n",
    "        item[\"timestamp\"] = item[\"timestamp\"] # timestamp doesn't need cleaning (it's totally fine :))\n",
    "                \n",
    "        item[\"num_likes\"] = likes_proc(item[\"num_likes\"]) # cleaning up the number of likes\n",
    "\n",
    "        item[\"parent_id\"] = parent_proc(item[\"parent_id\"]) # cleaning up the parent id\n",
    "        \n",
    "        return item \n",
    "    \n",
    "class user_privacy_proc(object):\n",
    "    \n",
    "    # this is meant to provide anonymization for the authors of comments as well as users who liked a post\n",
    "    def process_item(self, item, spider):\n",
    "            \n",
    "        # starting by making a function to create a UUID\n",
    "        def make_uuid(uname):\n",
    "            \n",
    "            uuid = shortuuid.uuid() # setting a uuid first\n",
    "        \n",
    "            # checking if the uuid is in the mapping alread -> unlikely but a good check\n",
    "            while uuid in spider.user_mapping.values():\n",
    "                uuid = shortuuid.uuid() # making a new uuid until it isn't in there -> hopefully won't take too long :(\n",
    "            \n",
    "            spider.user_mapping.setdefault(uname, uuid) # setting the uuid mapping\n",
    "            return uuid # returning the uuid\n",
    "        \n",
    "        # function to check a uuid\n",
    "        def check_uuid(uname):\n",
    "            \n",
    "            # first, check if a uname is in the mapping dict\n",
    "            if uname in spider.user_mapping:\n",
    "                return spider.user_mapping[uname] # returning that uuid\n",
    "            \n",
    "            else: # if it's not, we're going to make a mapping for it & return that new mapping\n",
    "                return make_uuid(uname)\n",
    "            \n",
    "        # if it's a comment\n",
    "        if isinstance(item, comment):\n",
    "            \n",
    "            # first check is if it's the author of the book\n",
    "            if item[\"comment_author\"] == \"erraticerrata\": # comment uname is lowercased\n",
    "                    return item # no proc needed if book author\n",
    "        \n",
    "            else: # if not, anonymize it\n",
    "                # first, we're getting the uuid\n",
    "                user_id = check_uuid(item[\"comment_author\"])\n",
    "\n",
    "                # then we're replacing the uname with the uuid\n",
    "                item[\"comment_author\"] = user_id\n",
    "                \n",
    "                return item\n",
    "        \n",
    "        # else it's a book_chapter\n",
    "        # here, we go through liked by\n",
    "        else:\n",
    "            unames = item[\"liked_by\"]\n",
    "            \n",
    "            # looping through each uname and replacing it with a uuid\n",
    "            for idx, uname in enumerate(unames):\n",
    "                unames[idx] = check_uuid(uname)\n",
    "            \n",
    "            # setting the liked_by to the unames list\n",
    "            item[\"liked_by\"] = unames\n",
    "            \n",
    "            # returning the item\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1251b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class book_spider(scrapy.Spider):\n",
    "    name = \"book scaper\"\n",
    "    start_urls = [\"https://practicalguidetoevil.wordpress.com/table-of-contents/\", \n",
    "                  \"https://practicalguidetoevil.wordpress.com/extra-chapters/\"] # the starting URLs we're working from\n",
    "    \n",
    "    # setting custom settings here\n",
    "    custom_settings = {\n",
    "        # first setting my feeds\n",
    "        'FEEDS' : { # setting the feed settings here \n",
    "            'book_chapters.jl': {\n",
    "                'format': 'jsonlines', \n",
    "                'overwrite': True,\n",
    "                'item_classes': [book_chapter], \n",
    "                'encoding': 'utf8', \n",
    "                'indent' : 4},\n",
    "\n",
    "                'comments.jl': {\n",
    "                'format': 'jsonlines', \n",
    "                'overwrite': True,\n",
    "                'item_classes': [comment],\n",
    "                'encoding': 'utf8' ,\n",
    "                'indent' : 4}\n",
    "                },\n",
    "        'LOG_LEVEL': logging.WARNING, # sets a min level for info to be written\n",
    "        'ITEM_PIPELINES': {\"__main__.chapter_proc_pipeline\": 1,\n",
    "                           \"__main__.comment_proc_pipeline\": 2,\n",
    "                          \"__main__.user_privacy_proc\" : 3}, # setting pipeline order\n",
    "        #  \"DOWNLOAD_DELAY\": .25, # the download delay -> here it's the default value for clarity\n",
    "        \"AUTOTHROTTLE_ENABLED \" : True, # the autothrottle being enables\n",
    "        \"AUTOTHROTTLE_START_DELAY\" : 1, # the starting delay for the autothrottle\n",
    "        \"AUTOTHROTTLE_MAX_DELAY\": 60, # the maximum value for the autothrottle -> up to 1min seems fair\n",
    "        'RETRY_HTTP_CODES' : [500, 502, 503, 504, 522, 524, 408, 429 , 404], # making a list of codes we'll retry on\n",
    "        'RETRY_TIMES': 5, # max retry times for a page; don't know that we'll need this here\n",
    "    }\n",
    "     \n",
    "    \n",
    "    user_mapping = dict() # creating an attribute to hold the user mapping for anonymization\n",
    "        \n",
    "    # creating the parsing method -> outer page\n",
    "    def parse(self, response):\n",
    "        \n",
    "        \n",
    "        # checking if it's the extra Chapters\n",
    "        if response.request.url == \"https://practicalguidetoevil.wordpress.com/extra-chapters/\":\n",
    "            book = response.css(\"h1.entry-title::text\").get()\n",
    "            chapters = response.css(\"div.entry-content > ul > li > a::attr('href')\").getall() # making a list of href chapters\n",
    "\n",
    "            for chapter in chapters:\n",
    "                chapt_info = book_chapter() # instantiating the book_chapter object\n",
    "                chapt_info[\"book\"] = book # the book is the book\n",
    "\n",
    "                # calling the secondary parse method here\n",
    "                yield scrapy.Request(chapter, callback = self.parse_chapt,\n",
    "                                        cb_kwargs={\"book_title\": book}) # passing through the data\n",
    "    \n",
    "        else:\n",
    "            books = response.css(\"div.entry-content > h2::text\").getall() # making a list of the book titles\n",
    "            chapters = response.css(\"div.entry-content > ul\") # making a list of uls\n",
    "\n",
    "            bc_zipped = tuple(zip(books, chapters)) # zipping those together\n",
    "\n",
    "            # looping through each of those in sync -> best way I'd say\n",
    "\n",
    "            for pair in bc_zipped:\n",
    "\n",
    "                book_title = pair[0] # setting the book title\n",
    "                book_chapters = pair[1].css(\"li > a::attr('href')\").getall() # pulling the hrefs -> list of them\n",
    "\n",
    "                for chapter_url in book_chapters: # for each url in each ul          \n",
    "\n",
    "                    # calling the secondary parse method here\n",
    "                    yield scrapy.Request(chapter_url, callback = self.parse_chapt,\n",
    "                                        cb_kwargs={\"book_title\": book_title}) # passing through the booktitle\n",
    "            \n",
    "    # creating the chapter parsing methods -> inner pages\n",
    "    def parse_chapt(self, response, book_title):\n",
    "        \n",
    "        chapt_info = book_chapter() # instantiating a book chapter\n",
    "        \n",
    "        chapt_info[\"book\"] = book_title # putting in the book title we pased through\n",
    "        \n",
    "        # pulling chapter title\n",
    "        chapt_info[\"chapt_title\"] = response.css(\"header.entry-header > h1::text\").get() \n",
    "        \n",
    "        # posted on datetime\n",
    "        chapt_info[\"published\"] = response.css(\"header > div > span.posted-on > a> time.entry-date.published::attr('datetime')\").get() \n",
    "        \n",
    "        # updated on datetime\n",
    "        chapt_info[\"updated\"] = response.css(\"header > div > span.posted-on > a > time.updated::attr('datetime')\").get()\n",
    "        \n",
    "        # byline = author\n",
    "        chapt_info[\"chapt_author\"] = response.css(\"header > div > span.byline > span.author.vcard > a::text\").get()\n",
    "        \n",
    "        # pulls the list of paragraphs that'll need to be joined\n",
    "        chapt_info[\"chapt_text\"] = response.css(\"div.entry-content > p ::text\").getall()\n",
    "                \n",
    "            \n",
    "        # pulls the number of comments\n",
    "        chapt_info[\"num_comments\"] = response.css(\"h2.comments-title::text\").get()\n",
    "        \n",
    "        # finding the post number\n",
    "        chapt_info[\"post_num\"] = response.css(\"input[name='comment_post_ID']::attr('value')\").get()\n",
    "        \n",
    "        # hmmm...it looks like my approach *should* be working here though\n",
    "        \n",
    "        # finding the url\n",
    "        likes_info = \"https://public-api.wordpress.com/rest/v1/batch?http_envelope=1&urls[]=/me&urls[]=/sites/87445915/posts/{}/likes&urls[]=/sites/87445915/posts/{}/reblogs/mine\".format(chapt_info[\"post_num\"],chapt_info[\"post_num\"])\n",
    "        \n",
    "        # yielding a new request to pull the likes info\n",
    "        yield scrapy.Request(likes_info, callback = self.parse_json_likes,\n",
    "                            cb_kwargs={\"chapt_info\":chapt_info}) # passing through the chapter info\n",
    "        \n",
    "        \n",
    "        # next we process the comment informaiton -> it's all on the chapter page\n",
    "        \n",
    "        # looping through each comment & parsing them\n",
    "        for article in response.css(\"article.comment\"):\n",
    "            \n",
    "            comment_item = comment() # instantiating a comment item\n",
    "            \n",
    "            # pulling comment author\n",
    "            comment_item[\"comment_author\"] = article.css(\"article > footer > div > cite ::text\").get() \n",
    "\n",
    "            # pulling the id\n",
    "            comment_item[\"comment_id\"] = article.css(\"article::attr('id')\").get()\n",
    "                        \n",
    "            # pulling the timestamp\n",
    "            comment_item[\"timestamp\"] = article.css(\"article > footer > div > a > time::attr('datetime')\").get()\n",
    "            \n",
    "            # pulling the text\n",
    "            comment_item[\"comment_text\"] = article.css(\"article > div > p::text\").getall()\n",
    "            \n",
    "            # pulling the number of likes\n",
    "            comment_item[\"num_likes\"] = article.css(\"article > div > p > span> a::text\").get()\n",
    "            \n",
    "            \n",
    "            # pulling the parent id -> may or may not exist\n",
    "            comment_item[\"parent_id\"] = article.xpath(\"../parent::ul/preceding-sibling::article/@id\").get()\n",
    "            \n",
    "            comment_item[\"post_num\"] = chapt_info[\"post_num\"]\n",
    "            \n",
    "            yield (comment_item) # returning the comment\n",
    "        \n",
    "    # parsing the json which has the like information we'll be using\n",
    "    def parse_json_likes(self, response, chapt_info):\n",
    "        \n",
    "        chapt_info[\"num_likes\"] = response.json()[\"body\"][\"/sites/87445915/posts/{}/likes\".format(chapt_info[\"post_num\"])][\"found\"]\n",
    "    \n",
    "        # the set of users is found here\n",
    "        lu = response.json()[\"body\"][\"/sites/87445915/posts/{}/likes\".format(chapt_info[\"post_num\"])][\"likes\"]        \n",
    "        \n",
    "        chapt_info[\"liked_by\"] = {item[\"nice_name\"] for item in lu} # set comp to pull unames\n",
    "        # remember to mention uncertainty around mapping comment unames to this set!!!\n",
    "            \n",
    "        yield chapt_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c079e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 16:20:37 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapybot)\n",
      "2022-10-03 16:20:37 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform Windows-10-10.0.19041-SP0\n",
      "2022-10-03 16:20:37 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_START_DELAY': 1,\n",
      " 'LOG_LEVEL': 30,\n",
      " 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408, 429, 404],\n",
      " 'RETRY_TIMES': 5,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 21s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(book_spider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd6607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
